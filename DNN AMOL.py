# -*- coding: utf-8 -*-
"""DNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vbKAyJ1KaqKgdyM7S6paylwv0rg_iQZm

for linear regression
"""

import numpy as np
import matplotlib.pyplot as plt

# Step 1: Generate synthetic dataset with a polynomial relationship
np.random.seed(0)  # for reproducibility
x = np.arange(0, 1.1, 0.1)  # Generate 11 points from 0 to 1
y_true = np.sin(2 * np.pi * x)

# Step 2: Add noise to the dataset
noise = np.random.normal(0, 0.1, x.shape)  # Gaussian noise with mean 0 and standard deviation 0.1
y_noisy = y_true + noise

# Step 3: Use polyfit function to fit a linear curve to the data
degree = 1  # Degree of polynomial to fit (linear regression)
coefficients = np.polyfit(x, y_noisy, degree)

# Step 4: Visualize the original data, the noisy data, and the linear regression fit
x_values = np.arange(0, 1.01, 0.01)  # For smoother curve visualization
y_linear_fit = np.polyval(coefficients, x_values)

plt.figure(figsize=(10, 6))
plt.plot(x, y_noisy, 'bo', label='Noisy Data')
plt.plot(x_values, np.sin(2 * np.pi * x_values), 'g-', label='True Function: sin(2πx)')
plt.plot(x_values, y_linear_fit, 'r-', label='Linear Regression Fit')
plt.title('Linear Regression on Noisy Data')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.grid(True)
plt.show()

"""polynomial curve fitlab1"""

#siddhimalusare20210802072

import numpy as np
import matplotlib.pyplot as plt

# Step 1: Generate synthetic dataset with a polynomial relationship
np.random.seed(0)  # for reproducibility
x = np.arange(0, 1.01, 0.02)  # Generate 50 points from 0 to 1
y_true = np.sin(2 * np.pi * x)

# Step 2: Add noise to the dataset
noise = np.random.normal(0, 0.1, x.shape)  # Gaussian noise with mean 0 and standard deviation 0.1
y_noisy = y_true + noise

# Step 3: Use polyfit function to fit a polynomial curve to the data
degree = 3  # Degree of polynomial to fit
coefficients = np.polyfit(x, y_noisy, degree)

# Step 4: Visualize the original data and the fitted curve
x_values = np.arange(0, 1.01, 0.01)  # For smoother curve visualization
y_fitted = np.polyval(coefficients, x_values)

plt.figure(figsize=(10, 6))
plt.plot(x, y_noisy, 'bo', label='Noisy Data')
plt.plot(x_values, np.sin(2 * np.pi * x_values), 'g-', label='True Function: sin(2πx)')
plt.plot(x_values, y_fitted, 'r-', label='Fitted Polynomial Curve (Degree {})'.format(degree))
plt.title('Polynomial Curve Fitting')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.grid(True)
plt.show()

"""SLP FOR CLASSIFICATION xor n xnor lab 2"""

#siddhimalusare20210802072
import numpy as np

# Define XOR and XNOR truth tables
X_classification = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y_classification_xor = np.array([0, 1, 1, 0])  # XOR
y_classification_xnor = np.array([1, 0, 0, 1])  # XNOR

# Step activation function
def step_function(x):
    return 1 if x >= 0 else 0

# Perceptron learning algorithm
def train_perceptron(X, y, epochs=1000, learning_rate=0.1):
    weights = np.random.rand(2)
    bias = np.random.rand(1)
    for _ in range(epochs):
        for i in range(len(X)):
            weighted_sum = np.dot(X[i], weights) + bias
            y_pred = step_function(weighted_sum)
            error = y[i] - y_pred
            weights += learning_rate * error * X[i]
            bias += learning_rate * error
    return weights, bias

# Train and evaluate the perceptron for XOR
weights_xor, bias_xor = train_perceptron(X_classification, y_classification_xor)
print("XOR Problem:")
for i in range(len(X_classification)):
    weighted_sum = np.dot(X_classification[i], weights_xor) + bias_xor
    y_pred = step_function(weighted_sum)
    print(f"Input: {X_classification[i]}, Predicted: {y_pred} (XOR)")

# Train and evaluate the perceptron for XNOR
weights_xnor, bias_xnor = train_perceptron(X_classification, y_classification_xnor)
print("XNOR Problem:")
for i in range(len(X_classification)):
    weighted_sum = np.dot(X_classification[i], weights_xnor) + bias_xnor
    y_pred = step_function(weighted_sum)
    print(f"Input: {X_classification[i]}, Predicted: {y_pred} (XNOR)")

"""SLP FOR REGRESSION

"""

import numpy as np

# Define XOR truth table
X_regression = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y_regression_xor = np.array([0, 1, 1, 0])  # XOR
y_regression_xnor = np.array([1, 0, 0, 1])  # XNOR

# Linear activation function for regression
def linear_function(x):
    return x

# Perceptron learning algorithm for regression
def train_perceptron_regression(X, y, weights, bias):
    learning_rate = 0.1
    epochs = 1000
    for _ in range(epochs):
        for i in range(len(X)):
            weighted_sum = np.dot(X[i], weights) + bias
            y_pred = linear_function(weighted_sum)
            error = y[i] - y_pred
            weights += learning_rate * error * X[i]
            bias += learning_rate * error
    return weights, bias

# Initialize weights and bias randomly
weights_regression = np.random.rand(2)
bias_regression = np.random.rand(1)

# Train the perceptron for XOR regression
weights_regression, bias_regression = train_perceptron_regression(X_regression, y_regression_xor, weights_regression, bias_regression)

# Evaluate the model for XOR regression
for i in range(len(X_regression)):
    weighted_sum = np.dot(X_regression[i], weights_regression) + bias_regression
    y_pred = linear_function(weighted_sum)
    print(f"Input: {X_regression[i]}, Predicted: {y_pred} (XOR)")

# Train the perceptron for XNOR regression
weights_regression, bias_regression = train_perceptron_regression(X_regression, y_regression_xnor, weights_regression, bias_regression)

# Evaluate the model for XNOR regression
for i in range(len(X_regression)):
    weighted_sum = np.dot(X_regression[i], weights_regression) + bias_regression
    y_pred = linear_function(weighted_sum)
    print(f"Input: {X_regression[i]}, Predicted: {y_pred} (XNOR)")

""" single perceptron for classification on AND &NAND truth table"""

import numpy as np
# Inputs for the truth tables
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])

# Outputs for AND and NAND truth tables
y_and = np.array([0, 0, 0, 1])  # AND
y_nand = np.array([1, 1, 1, 0])  # NAND
def step_function(x):
    return 1 if x >= 0 else 0
def step_function(x):
    return 1 if x >= 0 else 0
def train_perceptron(X, y, epochs=10, learning_rate=0.1):
    weights = np.random.rand(2)  # Initialize weights
    bias = np.random.rand(1)     # Initialize bias

    for _ in range(epochs):
        for i in range(len(X)):
            # Compute the weighted sum
            weighted_sum = np.dot(X[i], weights) + bias
            # Determine the prediction using the step function
            y_pred = step_function(weighted_sum)
            # Compute the error
            error = y[i] - y_pred
            # Update weights and bias
            weights += learning_rate * error * X[i]
            bias += learning_rate * error
    return weights, bias
# Train the perceptron for AND
weights_and, bias_and = train_perceptron(X, y_and)

# Evaluate the perceptron for AND
print("AND Gate Predictions:")
for i in range(len(X)):
    weighted_sum = np.dot(X[i], weights_and) + bias_and
    y_pred = step_function(weighted_sum)
    print(f"Input: {X[i]}, Predicted: {y_pred}")

# Train the perceptron for NAND
weights_nand, bias_nand = train_perceptron(X, y_nand)

# Evaluate the perceptron for NAND
print("NAND Gate Predictions:")
for i in range(len(X)):
    weighted_sum = np.dot(X[i], weights_nand) + bias_nand
    y_pred = step_function(weighted_sum)
    print(f"Input: {X[i]}, Predicted: {y_pred}")

""" multiple perceptron for classification on XOR & XNOR truth table"""

import numpy as np
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input dataset
y_xor = np.array([[0], [1], [1], [0]])  # XOR output
y_xnor = np.array([[1], [0], [0], [1]])  # XNOR output (inverse of XOR)
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)
np.random.seed(0)  # Seed for reproducibility

input_size = X.shape[1]  # Number of input features (2)
hidden_size = 2  # Number of neurons in the hidden layer
output_size = 1  # Number of neurons in the output layer (1 for binary classification)

weights_input_hidden = np.random.uniform(size=(input_size, hidden_size))
weights_hidden_output = np.random.uniform(size=(hidden_size, output_size))
bias_hidden = np.random.uniform(size=(1, hidden_size))
bias_output = np.random.uniform(size=(1, output_size))
learning_rate = 0.5
epochs = 10000

for epoch in range(epochs):
    # Forward Pass
    hidden_layer_input = np.dot(X, weights_input_hidden) + bias_hidden
    hidden_layer_activation = sigmoid(hidden_layer_input)
    output_layer_input = np.dot(hidden_layer_activation, weights_hidden_output) + bias_output
    predicted_output = sigmoid(output_layer_input)

    # Backward Pass
    error = y_xor - predicted_output
    d_predicted_output = error * sigmoid_derivative(predicted_output)

    error_hidden_layer = d_predicted_output.dot(weights_hidden_output.T)
    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_activation)

    # Updating Weights and Biases
    weights_hidden_output += hidden_layer_activation.T.dot(d_predicted_output) * learning_rate
    bias_output += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate
    weights_input_hidden += X.T.dot(d_hidden_layer) * learning_rate
    bias_hidden += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate

    if epoch % 1000 == 0:
        loss = np.mean(np.square(y_xor - predicted_output)) # Mean Squared Error
        print(f"Epoch {epoch} Loss: {loss}")
print("Predicted Outputs for XOR:")
print(predicted_output > 0.5)  # Applying threshold to convert probabilities to binary output

"""multiple perceptron for classification on AND & NAND truth table"""

import numpy as np
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input dataset
y_and = np.array([[0], [0], [0], [1]])  # AND output
y_nand = np.array([[1], [1], [1], [0]])  # NAND output
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)
np.random.seed(1)  # Seed for reproducibility

input_size = X.shape[1]  # Number of input features (2)
hidden_size = 2  # Number of neurons in the hidden layer
output_size = 1  # Number of output neurons (1 for binary classification)

weights_input_hidden = np.random.uniform(low=-1, high=1, size=(input_size, hidden_size))
weights_hidden_output = np.random.uniform(low=-1, high=1, size=(hidden_size, output_size))
bias_hidden = np.random.uniform(low=-1, high=1, size=(1, hidden_size))
bias_output = np.random.uniform(low=-1, high=1, size=(1, output_size))
learning_rate = 0.1
epochs = 10000

for epoch in range(epochs):
    # Forward Pass
    hidden_layer_input = np.dot(X, weights_input_hidden) + bias_hidden
    hidden_layer_activation = sigmoid(hidden_layer_input)
    output_layer_input = np.dot(hidden_layer_activation, weights_hidden_output) + bias_output
    predicted_output = sigmoid(output_layer_input)

    # Backward Pass
    error = y_and - predicted_output
    d_predicted_output = error * sigmoid_derivative(predicted_output)

    error_hidden_layer = d_predicted_output.dot(weights_hidden_output.T)
    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_activation)

    # Updating Weights and Biases
    weights_hidden_output += hidden_layer_activation.T.dot(d_predicted_output) * learning_rate
    bias_output += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate
    weights_input_hidden += X.T.dot(d_hidden_layer) * learning_rate
    bias_hidden += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate

    if epoch % 1000 == 0:
        loss = np.mean(np.square(y_and - predicted_output))  # Mean Squared Error
        print(f"Epoch {epoch} Loss: {loss}")
print("Predicted Outputs for AND:")
print(predicted_output > 0.5)  # Applying threshold to convert probabilities to binary output

"""for neural network lab3"""

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

dp=pd.read_csv('iris.data.csv')
print(dp)

X_iris = iris.data
y_iris = iris.target

# Select first four features from Iris dataset
selected_features = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']

# Create a DataFrame with selected features
df_iris = pd.DataFrame(X_iris, columns=selected_features)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df_iris, y_iris, test_size=0.3, random_state=42)

print(X_train, y_train)

scaler = StandardScaler()

# Fit and transform the training data
X_train_scaled = scaler.fit_transform(X_train)

# Transform the testing data using the same scaler
X_test_scaled = scaler.transform(X_test)

X_test_scaled = scaler.transform(X_test)

model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(X_train_scaled.shape[1],)),
    tf.keras.layers.Dense(120, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(3, activation='softmax')  # Output layer with 3 units for Iris dataset
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train_scaled, y_train, epochs=50, validation_split=0.2, verbose=1)

# Evaluate the model on test data
test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)
print("Test Accuracy:", test_accuracy)
model.summary()


import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

""" neural network from scratch using a high level library like tensorflow or Pytorch for prediction. Use appropriate dataset"""

import numpy as np
import tensorflow as tf
import pandas as pd
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load the dataset
(x_train_full, y_train_full), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data()

# Split the full training data into a validation set and a smaller training set
x_train, x_valid, y_train, y_valid = train_test_split(x_train_full, y_train_full, test_size=0.2, random_state=42)

# Convert training data to DataFrame for better visualization
df_train = pd.DataFrame(x_train, columns=["CRIM", "ZN", "INDUS", "CHAS", "NOX", "RM", "AGE", "DIS", "RAD", "TAX", "PTRATIO", "B", "LSTAT"])
df_train['MEDV'] = y_train  # Add the target variable to the DataFrame

# Show the first few rows of the DataFrame
print(df_train.head())




# Standardize the data (important for neural networks)
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_valid = scaler.transform(x_valid)
x_test = scaler.transform(x_test)
model = models.Sequential([
    layers.Dense(64, activation='relu', input_shape=(x_train.shape[1],)),
    layers.Dense(64, activation='relu'),
    layers.Dense(1)  # Output layer: no activation function as it's a regression problem
])
model.compile(optimizer='adam', loss='mse', metrics=['mae'])
history = model.fit(x_train, y_train, epochs=100, validation_data=(x_valid, y_valid))
test_loss, test_mae = model.evaluate(x_test, y_test)
print("Test Mean Absolute Error: ", test_mae)
predictions = model.predict(x_test[:5])
print("Predictions on Test Data:", predictions.flatten())
print("Actual Values:", y_test[:5])

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load Iris dataset
iris = load_iris()

# Extract features and target variable
X_iris = iris.data
y_iris = iris.target

# Select first four features from Iris dataset
selected_features = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']

# Create a DataFrame with selected features
df_iris = pd.DataFrame(X_iris, columns=selected_features)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df_iris, y_iris, test_size=0.3, random_state=42)

scaler = StandardScaler()

# Fit and transform the training data
X_train_scaled = scaler.fit_transform(X_train)

# Transform the testing data using the same scaler
X_test_scaled = scaler.transform(X_test)

#siddhimalusare20210802072
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score

# Load the dataset with explicit column names
data = pd.read_csv('iris.data.csv', names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'])

# Check if the target column 'species' exists in the DataFrame
if 'species' not in data.columns:
    raise ValueError("Target column 'species' not found in the dataset.")

# Preprocess the data
# Assuming 'data' is your DataFrame containing the dataset
X = data.drop('species', axis=1).values
y = data['species'].values

# Encode the labels
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Build the neural network model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(10, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(3, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1)

# Evaluate the model
y_pred_probabilities = model.predict(X_test)
y_pred = np.argmax(y_pred_probabilities, axis=1)
accuracy = accuracy_score(y_test, y_pred)
print("Test Accuracy:", accuracy)

pip install keras-tuner

"""LAB4 HYPERPARAMETER"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.datasets import fashion_mnist
from kerastuner.tuners import RandomSearch  # Import RandomSearch from keras-tuner

# Load the Fashion MNIST dataset
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()

# Visualize the dataset
for i in range(25):
    plt.subplot(5, 5, i+1)
    plt.imshow(X_train[i], cmap=plt.get_cmap('gray'))

plt.show()

# Normalize the images
X_train = X_train / 255
X_test = X_test / 255

# Define the model
def build_model(hp):
    model = Sequential()
    model.add(Flatten(input_shape=(28, 28)))
    # Number of neurons in a hidden layer
    model.add(Dense(units=hp.Int('num_of_neurons', min_value=32, max_value=512, step=32), activation='relu'))
    model.add(Dense(10, activation='softmax'))
    # Compile the model
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Initialize RandomSearch tuner
tuner = RandomSearch(build_model,
                     objective='val_accuracy',
                     max_trials=5,
                     executions_per_trial=3,
                     directory='tuner1',
                     project_name='Clothing')

# Search space summary
tuner.search_space_summary()

# Fit the tuner on train dataset
tuner.search(X_train, y_train, epochs=10, validation_data=(X_test, y_test))

# Summary of results
tuner.results_summary()

"""9)	Optimize hyperparameters for a neural network model. Implement a hyperparameter optimization strategy and compare the performance with different hyperparameter configurations for classification (using keras tuner)"""

import tensorflow as tf
from tensorflow import keras
from kerastuner import RandomSearch
from kerastuner.engine.hyperparameters import HyperParameters
fashion_mnist = keras.datasets.fashion_mnist
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

# Normalize the data
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0
def build_model(hp):
    model = keras.Sequential()
    model.add(keras.layers.Flatten(input_shape=(28, 28)))
    # Tune the number of units in the first Dense layer
    # Choose an optimal value between 32-512
    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)
    model.add(keras.layers.Dense(units=hp_units, activation='relu'))
    model.add(keras.layers.Dense(10, activation='softmax'))

    # Tune the learning rate for the optimizer
    # Choose an optimal value from 0.01, 0.001, or 0.0001
    hp_learning_rate = hp.Choice('learning_rate', values=[0.01, 0.001, 0.0001])

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=10,  # Set the number of trials (different model configurations) to test
    executions_per_trial=1,
    directory='project_dir',
    project_name='fashion_mnist'
)

tuner.search(x_train, y_train, epochs=10, validation_split=0.2)
best_model = tuner.get_best_models(num_models=1)[0]
test_loss, test_acc = best_model.evaluate(x_test, y_test)
print(f"Best model accuracy: {test_acc}")

"""10) Implement a hyperparameter optimization strategy and compare the performance with different hyperparameter configurations for regression (using keras tuner)"""

import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from tensorflow import keras
from keras_tuner import RandomSearch, HyperParameters
import matplotlib.pyplot as plt

# Load MNIST dataset
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Normalize the data
x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255
x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255

# Convert labels to categorical (one-hot encoding)
y_train = keras.utils.to_categorical(y_train, 10)
y_test = keras.utils.to_categorical(y_test, 10)

# Show an example image
plt.figure(figsize=(2, 2))
plt.imshow(x_train[0].reshape(28, 28), cmap='gray')
plt.title(f"Example Image - Label: {np.argmax(y_train[0])}")
plt.colorbar()
plt.show()

# Define the model building function for the tuner
def build_model(hp):
    model = keras.Sequential([
        keras.layers.Conv2D(
            filters=hp.Int('filters', min_value=32, max_value=128, step=32),
            kernel_size=hp.Choice('kernel_size', values=[3, 5]),
            activation='relu',
            input_shape=(28, 28, 1)
        ),
        keras.layers.MaxPooling2D(pool_size=(2, 2)),
        keras.layers.Flatten(),
        keras.layers.Dense(
            units=hp.Int('units', min_value=32, max_value=128, step=32),
            activation='relu'
        ),
        keras.layers.Dense(10, activation='softmax')
    ])

    hp_learning_rate = hp.Choice('learning_rate', values=[0.01, 0.001, 0.0001])
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

# Setup the tuner
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',  # Maximize validation accuracy
    max_trials=10,  # Limit the number of trials to test
    executions_per_trial=1,
    directory='project_dir',
    project_name='mnist_classification'
)

# Perform the hyperparameter search
tuner.search(x_train, y_train, epochs=10, validation_split=0.1)

# Get the best model
best_model = tuner.get_best_models(num_models=1)[0]

# Evaluate the best model on test data
test_loss, test_acc = best_model.evaluate(x_test, y_test)
print(f"Best model accuracy on test data: {test_acc}")

"""11)	Optimize hyperparameters for a neural network model. Implement a hyperparameter optimization strategy and compare the performance with different hyperparameter configurations for classification (without keras tuner)"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
import random
import matplotlib.pyplot as plt
# Load MNIST dataset
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Normalize the data
x_train = x_train.reshape(-1, 28 * 28).astype('float32') / 255
x_test = x_test.reshape(-1, 28 * 28).astype('float32') / 255

# Convert labels to one-hot encoding
y_train = keras.utils.to_categorical(y_train, 10)
y_test = keras.utils.to_categorical(y_test, 10)
def create_model(n_units, learning_rate, activation):
    model = keras.Sequential([
        keras.layers.Dense(n_units, activation=activation, input_shape=(784,)),
        keras.layers.Dense(10, activation='softmax')
    ])
    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model
n_units_options = [32, 64, 128, 256]
learning_rates = [0.01, 0.001, 0.0001]
activations = ['relu', 'tanh']
best_acc = 0
best_params = {}

for _ in range(10):  # Number of random samples
    n_units = random.choice(n_units_options)
    learning_rate = random.choice(learning_rates)
    activation = random.choice(activations)

    model = create_model(n_units, learning_rate, activation)
    model.fit(x_train, y_train, epochs=5, batch_size=128, verbose=0)
    _, acc = model.evaluate(x_test, y_test, verbose=0)

    print(f"Tested: Units={n_units}, LR={learning_rate}, Activation={activation}, Accuracy={acc:.4f}")

    if acc > best_acc:
        best_acc = acc
        best_params = {'n_units': n_units, 'learning_rate': learning_rate, 'activation': activation}

print(f"Best Hyperparameters: {best_params} with Accuracy: {best_acc:.4f}")

"""12)	Optimize hyperparameters for a neural network model. Implement a hyperparameter optimization strategy and compare the performance with different hyperparameter configurations for regression (without keras tuner)"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
import random
# Load MNIST dataset
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Normalize the data
x_train = x_train.reshape(-1, 28 * 28).astype('float32') / 255
x_test = x_test.reshape(-1, 28 * 28).astype('float32') / 255

# Labels do not need conversion to one-hot encoding; they are treated as continuous values
y_train = y_train.astype('float32')
y_test = y_test.astype('float32')
def create_model(n_units, learning_rate):
    model = keras.Sequential([
        keras.layers.Dense(n_units, activation='relu', input_shape=(784,)),
        keras.layers.Dense(n_units, activation='relu'),
        keras.layers.Dense(1)  # Output layer for regression
    ])
    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='mse',  # Mean Squared Error for regression
                  metrics=['mae'])  # Mean Absolute Error as an additional metric
    return model
n_units_options = [32, 64, 128]
learning_rates = [0.01, 0.001, 0.0001]
best_mae = float('inf')
best_params = {}

for _ in range(20):  # Number of trials
    n_units = random.choice(n_units_options)
    learning_rate = random.choice(learning_rates)

    model = create_model(n_units, learning_rate)
    model.fit(x_train, y_train, epochs=10, batch_size=128, verbose=0)
    _, mae = model.evaluate(x_test, y_test, verbose=0)

    print(f"Tested: Units={n_units}, LR={learning_rate}, MAE={mae:.4f}")

    if mae < best_mae:
        best_mae = mae
        best_params = {'n_units': n_units, 'learning_rate': learning_rate}

print(f"Best Hyperparameters: {best_params} with MAE: {best_mae:.4f}")

"""13)	Compute the Hessian matrix for a given scalar-valued function of multiple variables."""

!pip install sympy
import sympy as sp

# Define the variables
x, y, z = sp.symbols('x y z')

# Define the function
f = x**2 * y + y**2 * z + z**2 * x

# Compute the Hessian matrix
hessian_matrix = sp.hessian(f, (x, y, z))

# Display the Hessian matrix
print("Hessian Matrix of the function:")
sp.pprint(hessian_matrix)

# Optionally, you can evaluate the Hessian matrix at a specific point, e.g., (x=1, y=1, z=1)
hessian_matrix_at_point = hessian_matrix.subs({x: 1, y: 1, z: 1})
print("\nHessian Matrix at (x=1, y=1, z=1):")
sp.pprint(hessian_matrix_at_point)

"""14)	Implement Bayesian methods for classification using Gaussian"""

!pip install numpy scikit-learn matplotlib
import numpy as np
from sklearn.datasets import make_classification
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import matplotlib.pyplot as plt
X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# The length_scale parameter controls the smoothness of the resulting function
kernel = 1.0 * RBF(length_scale=1.0)
gpc = GaussianProcessClassifier(kernel=kernel, random_state=42)

# Fit the Gaussian Process Classifier model
gpc.fit(X_train, y_train)
# Predictions on the test set
y_pred = gpc.predict(X_test)
probabilities = gpc.predict_proba(X_test)[:, 1]

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("Classification Report:\n", classification_report(y_test, y_pred))

# Optionally, plot the probability densities
plt.figure(figsize=(10, 6))
plt.hist(probabilities[y_test == 0], color='r', alpha=0.5, label='Class 0')
plt.hist(probabilities[y_test == 1], color='b', alpha=0.5, label='Class 1')
plt.title('Probability Distributions by Predicted Class')
plt.xlabel('Probability')
plt.ylabel('Density')
plt.legend()
plt.show()

from sklearn.metrics import f1_score

# Calculate accuracy and F1 score
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='binary')  # Use 'binary' for binary classification

print("Accuracy:", accuracy)
print("F1 Score:", f1)

# Full classification report, which includes precision, recall, and F1-score for each class
print("Classification Report:\n", classification_report(y_test, y_pred))

"""15)	Implement Bayesian methods for classification using Naïve Bayes"""

from sklearn.datasets import make_classification
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB

# Generate synthetic classification data
X, y = make_classification(
    n_features=6,
    n_classes=3,
    n_samples=1000,
    n_informative=2,
    random_state=1,
    n_clusters_per_class=1
)

# Visualize the data
plt.scatter(X[:, 0], X[:, 1], c=y, marker="*")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.title("Scatter plot of synthetic classification data")
plt.show()

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=125)

# Build a Gaussian Naive Bayes classifier
model = GaussianNB()

# Train the model
model.fit(X_train, y_train)

# Predict output
predicted = model.predict([X_test[6]])

print("Actual Value:", y_test[5])
print("Predicted Value:", predicted[0])

from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, f1_score

# Predictions
y_pred = model.predict(X_test)

# Accuracy
accuracy = accuracy_score(y_pred, y_test)

# F1 Score
f1 = f1_score(y_pred, y_test, average="weighted")

print("Accuracy:", accuracy)
print("F1 Score:", f1)

"""16)	Implement Principal component analysis (02 components) for dimensionality reduction of data points"""

#20210802072 siddhi malusare
import numpy as np
import matplotlib.pyplot as plt

# 1. Load iris dataset as an example
from sklearn.datasets import load_iris

iris = load_iris()
X = iris.data
y = iris.target

# Scatter plot before PCA
plt.figure(figsize=(8, 6))
for i in range(len(iris.target_names)):
    plt.scatter(X[y == i, 0], X[y == i, 1], label=iris.target_names[i])

plt.title('Scatter Plot Before PCA')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.grid(True)
plt.show()

# 2. Standardize the data
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 3. Implement PCA Algorithm
from sklearn.decomposition import PCA

pca = PCA()

# 4. Calculate the cumulative explained variance
pca.fit(X_scaled)
cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)

# 5. Determine the number of components to keep for given variance
target_variance = 0.95
num_components = np.argmax(cumulative_explained_variance >= target_variance) + 1

print("Number of components to keep for {:.0f}% variance: {}".format(target_variance * 100, num_components))

# 6. Apply PCA with the selected number of components
pca = PCA(n_components=num_components)
X_pca = pca.fit_transform(X_scaled)

print("Original data shape:", X.shape)
print("Transformed data shape after PCA:", X_pca.shape)

# Scatter plot after PCA
plt.figure(figsize=(8, 6))
for i in range(len(iris.target_names)):
    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], label=iris.target_names[i])

plt.title('Scatter Plot After PCA')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()
plt.grid(True)
plt.show()

"""17)	Implement Principal component analysis (04 components) for dimensionality reduction of data points"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
# Load dataset
iris = load_iris()
X = iris.data
y = iris.target

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
# Apply PCA
pca = PCA(n_components=4)
X_pca = pca.fit_transform(X_scaled)

# Print the variance ratio to see how much information we've captured
explained_variance = pca.explained_variance_ratio_
print("Explained variance ratio:", explained_variance)
print("Cumulative explained variance:", np.cumsum(explained_variance))
# Plotting the first two principal components
plt.figure(figsize=(8, 6))
for i, target_name in enumerate(iris.target_names):
    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], alpha=0.8, label=target_name)
plt.title('PCA of Iris Dataset (1st and 2nd Principal Components)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()
plt.grid(True)
plt.show()

# Plotting the third and fourth principal components
plt.figure(figsize=(8, 6))
for i, target_name in enumerate(iris.target_names):
    plt.scatter(X_pca[y == i, 2], X_pca[y == i, 3], alpha=0.8, label=target_name)
plt.title('PCA of Iris Dataset (3rd and 4th Principal Components)')
plt.xlabel('Principal Component 3')
plt.ylabel('Principal Component 4')
plt.legend()
plt.grid(True)
plt.show()

"""if variance is asked"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Load dataset
iris = load_iris()
X = iris.data
y = iris.target

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply PCA with four components
pca = PCA(n_components=4)
X_pca = pca.fit_transform(X_scaled)

# Calculate the cumulative explained variance
cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)

# Plotting the result
plt.figure(figsize=(12, 6))

# Visualizing the data with the first two of four principal components
plt.subplot(1, 2, 1)
for i, target_name in enumerate(iris.target_names):
    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], alpha=0.8, label=target_name)
plt.title('PCA of Iris Dataset (First 2 of 4 Principal Components)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()
plt.grid(True)

# Plotting the cumulative explained variance to show the cutoff for four components
plt.subplot(1, 2, 2)
plt.plot(cumulative_explained_variance)
plt.axhline(y=0.95, color='r', linestyle='--', label='95% Variance')
plt.axvline(x=3, color='r', linestyle='--', label='4 Components')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Explained Variance')
plt.legend()

plt.show()

# Print the cumulative explained variance captured by the first four components
print("Cumulative explained variance by 4 components:", cumulative_explained_variance[3])

"""18)	Implement Hidden Markov Model for sequence prediction and evaluate model performance(Stock Market)

"""

!pip install hmmlearn
!pip install yfinance
import numpy as np
from hmmlearn import hmm
import yfinance as yf

# Fetch historical stock data for Apple Inc. (AAPL)
stock_data = yf.download('AAPL', start='2020-01-01', end='2022-01-01')
# Calculate daily price changes
stock_data['Price_Change'] = stock_data['Close'].pct_change().fillna(0)

# Define states: 'Increase', 'Decrease', 'No Change'
states = ['Increase', 'Decrease', 'No Change']

# Encode price changes into states
def encode_states(price_change):
    if price_change > 0:
        return 0  # Increase
    elif price_change < 0:
        return 1  # Decrease
    else:
        return 2  # No Change

stock_data['State'] = stock_data['Price_Change'].apply(encode_states)
# Prepare training data
X_train = stock_data['State'].values.reshape(-1, 1)

# Initialize and train HMM model
model = hmm.MultinomialHMM(n_components=3, n_iter=100)
model.fit(X_train)
# Predict next day's state
predicted_state = model.predict(X_train[-1].reshape(1, -1))
predicted_movement = states[predicted_state[0]]

print("Predicted Movement for Next Trading Day:", predicted_movement)

"""19)	Implement Hidden Markov Model for sequence prediction and evaluate model performance(Weather prediction)"""

!pip install hmmlearn

import numpy as np
from hmmlearn import hmm
from sklearn.preprocessing import LabelEncoder

# Generate a synthetic weather dataset
weather = ['sunny', 'rainy', 'cloudy']
np.random.seed(0)  # For reproducibility
data = np.random.choice(weather, size=100)

# Preprocessing: Encode weather categories
weather_encoder = LabelEncoder()
encoded_data = weather_encoder.fit_transform(data).reshape(-1, 1)

# Split data into training and testing sets
train_size = int(0.8 * len(encoded_data))
X_train = encoded_data[:train_size]
X_test = encoded_data[train_size:]

# Initialize and train the HMM model
model = hmm.MultinomialHMM(n_components=3, n_iter=100)  # 3 weather states: sunny, rainy, cloudy
model.fit(X_train)

# Predict the most probable state sequence for the test set
hidden_states = model.predict(X_test)

# Evaluation using log likelihood
log_likelihood_train = model.score(X_train)
log_likelihood_test = model.score(X_test)

print("Log Likelihood (Train):", log_likelihood_train)
print("Log Likelihood (Test):", log_likelihood_test)

# Predict the next day's weather
# Assuming the last known state is the last observation in our dataset
last_observation = encoded_data[-1].reshape(1, -1)
next_day_state = model.predict(last_observation)

# Decode the predicted state back to weather label
predicted_weather = weather_encoder.inverse_transform(next_day_state)

print("Predicted Weather for Next Day:", predicted_weather[0])

"""20)	Implement Hidden Markov Model for sequence prediction and evaluate model performance(Supply chain )"""

!pip install hmmlearn numpy matplotlib
!pip install --upgrade hmmlearn

import numpy as np
from hmmlearn import hmm
import matplotlib.pyplot as plt

# Function to simulate data
def simulate_data(num_days):
    # States are Low, Medium, High inventory
    states = ["Low", "Medium", "High"]
    n_states = len(states)

    # Daily sales volume fluctuations: Low, Medium, High
    sales_volumes = ["Low", "Medium", "High"]
    n_observations = len(sales_volumes)

    # Start probabilities for inventory states
    start_prob = np.array([0.1, 0.3, 0.6])

    # Transition probability matrix between inventory states
    trans_prob = np.array([
        [0.6, 0.3, 0.1],  # From Low to Low, Medium, High
        [0.2, 0.5, 0.3],  # From Medium to Low, Medium, High
        [0.1, 0.3, 0.6]   # From High to Low, Medium, High
    ])

    # Probability of sales volume given inventory state
    sales_prob = np.array([
        [0.7, 0.2, 0.1],  # High sales probability when inventory is Low
        [0.3, 0.4, 0.3],  # Even distribution when inventory is Medium
        [0.1, 0.2, 0.7]   # Low sales probability when inventory is High
    ])

    # Create the HMM
    model = hmm.MultinomialHMM(n_components=n_states, random_state=42)
    model.startprob_ = start_prob
    model.transmat_ = trans_prob
    model.emissionprob_ = sales_prob

    # Simulate data
    inventory, sales = model.sample(num_days)

    return states, sales_volumes, model, inventory, sales

# Simulate data for 365 days
states, sales_volumes, model, inventory, sales = simulate_data(365)

# Visualize the inventory levels
plt.figure(figsize=(10, 4))
plt.plot(inventory[:100], 'o-', label='Inventory Level')
plt.xticks(ticks=np.arange(100), labels=sales[:100], rotation=90)
plt.title('Simulated Inventory Levels Over Time')
plt.xlabel('Days')
plt.ylabel('Inventory Level')
plt.legend()
plt.show()

# Calculate log likelihood for the model on the observed data
log_likelihood = model.score(inventory)
print("Log Likelihood of the model:", log_likelihood)

# Assume we have another set of data for testing
_, _, _, test_inventory, _ = simulate_data(100)
log_likelihood_test = model.score(test_inventory)
print("Log Likelihood on test data:", log_likelihood_test)