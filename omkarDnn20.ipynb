{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKGVRvYZDT0n"
      },
      "source": [
        "**1) Implement Polynomial Curve fitting using the polyfit function for linear regression code a simple and simplifuied code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "e8x5HooJDM4G",
        "outputId": "25d90654-b9b7-445a-e8ce-7aec9d1305bc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.linspace(0, 10, 15)\n",
        "y = 2 * x**2 + 3 * x + 1 + np.random.normal(0, 5, 15)\n",
        "\n",
        "coefficients = np.polyfit(x, y, 1)\n",
        "poly_function = np.poly1d(coefficients)\n",
        "\n",
        "# Plot original data and fitted polynomial curve\n",
        "plt.scatter(x, y, label='Original Data')\n",
        "plt.plot(x, poly_function(x), color='red', label='Fitted Polynomial Curve')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Polynomial Curve Fitting')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0LeqhEfDwT6"
      },
      "source": [
        "**2)\tImplement Polynomial Curve fitting using the polyfit function for linear regression when added with noise**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "dPHxtwuZDfPv",
        "outputId": "9c427ad9-22ba-47af-fb1f-9bcac182157e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_polynomial_curve(x, y, degree, noise_std):\n",
        "    y_noisy = y + np.random.normal(0, noise_std, len(y))\n",
        "    \n",
        "    coefficients = np.polyfit(x, y_noisy, degree)\n",
        "    poly_function = np.poly1d(coefficients)\n",
        "    \n",
        "    plt.scatter(x, y_noisy, label='Original Data with Noise')\n",
        "    plt.plot(x, poly_function(x), color='red', label=f'Fitted Polynomial Curve (degree={degree})')\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('Y')\n",
        "    plt.title('Polynomial Curve Fitting')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "x = np.linspace(0, 10, 15)\n",
        "y = 2 * x**2 + 3 * x + 1\n",
        "degree = 2\n",
        "noise_std = 5\n",
        "\n",
        "plot_polynomial_curve(x, y, degree, noise_std)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGCEG-1YEbfB"
      },
      "source": [
        "**3)Implement a basic single perceptron for regression on XOR & XNOR truth table**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3dirC-8D-gZ",
        "outputId": "f89cdf6f-d4ae-4291-b290-abca04e44c15"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "# XOR truth table\n",
        "X_xor = np.array([[0, 0],\n",
        "                  [0, 1],\n",
        "                  [1, 0],\n",
        "                  [1, 1]])\n",
        "y_xor = np.array([0, 1, 1, 0])\n",
        "\n",
        "# XNOR truth table\n",
        "X_xnor = np.array([[0, 0],\n",
        "                   [0, 1],\n",
        "                   [1, 0],\n",
        "                   [1, 1]])\n",
        "y_xnor = np.array([1, 0, 0, 1])\n",
        "\n",
        "# Create and train perceptron for XOR\n",
        "perceptron_xor = Perceptron(max_iter=5000, tol=1e-3)\n",
        "perceptron_xor.fit(X_xor, y_xor)\n",
        "\n",
        "# Create and train perceptron for XNOR\n",
        "perceptron_xnor = Perceptron(max_iter=1750, tol=1e-3)\n",
        "perceptron_xnor.fit(X_xnor, y_xnor)\n",
        "\n",
        "# Predictions\n",
        "print(\"Predictions for XOR:\")\n",
        "print(\"0 XOR 0:\", perceptron_xor.predict([[0, 0]]))\n",
        "print(\"0 XOR 1:\", perceptron_xor.predict([[0, 1]]))\n",
        "print(\"1 XOR 0:\", perceptron_xor.predict([[1, 0]]))\n",
        "print(\"1 XOR 1:\", perceptron_xor.predict([[1, 1]]))\n",
        "\n",
        "print(\"\\nPredictions for XNOR:\")\n",
        "print(\"0 XNOR 0:\", perceptron_xnor.predict([[0, 0]]))\n",
        "print(\"0 XNOR 1:\", perceptron_xnor.predict([[0, 1]]))\n",
        "print(\"1 XNOR 0:\", perceptron_xnor.predict([[1, 0]]))\n",
        "print(\"1 XNOR 1:\", perceptron_xnor.predict([[1, 1]]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qvko_r6FEzB"
      },
      "source": [
        "**4)Implement a basic single perceptron for classification on AND &NAND truth table**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzsSG7XOEu1G",
        "outputId": "e45cf0fa-9124-439f-e482-ac01c2f1fbe2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "# AND truth table\n",
        "X_and = np.array([[0, 0],\n",
        "                  [0, 1],\n",
        "                  [1, 0],\n",
        "                  [1, 1]])\n",
        "y_and = np.array([0, 0, 0, 1])\n",
        "\n",
        "# NAND truth table\n",
        "X_nand = np.array([[0, 0],\n",
        "                   [0, 1],\n",
        "                   [1, 0],\n",
        "                   [1, 1]])\n",
        "y_nand = np.array([1, 1, 1, 0])\n",
        "\n",
        "# Create and train perceptron for AND\n",
        "perceptron_and = Perceptron(max_iter=1000, tol=1e-3)\n",
        "perceptron_and.fit(X_and, y_and)\n",
        "\n",
        "# Create and train perceptron for NAND\n",
        "perceptron_nand = Perceptron(max_iter=1000, tol=1e-3)\n",
        "perceptron_nand.fit(X_nand, y_nand)\n",
        "\n",
        "# Predictions\n",
        "print(\"Predictions for AND:\")\n",
        "print(\"0 AND 0:\", perceptron_and.predict([[0, 0]]))\n",
        "print(\"0 AND 1:\", perceptron_and.predict([[0, 1]]))\n",
        "print(\"1 AND 0:\", perceptron_and.predict([[1, 0]]))\n",
        "print(\"1 AND 1:\", perceptron_and.predict([[1, 1]]))\n",
        "\n",
        "print(\"\\nPredictions for NAND:\")\n",
        "print(\"0 NAND 0:\", perceptron_nand.predict([[0, 0]]))\n",
        "print(\"0 NAND 1:\", perceptron_nand.predict([[0, 1]]))\n",
        "print(\"1 NAND 0:\", perceptron_nand.predict([[1, 0]]))\n",
        "print(\"1 NAND 1:\", perceptron_nand.predict([[1, 1]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zTw5UkBFUTC"
      },
      "source": [
        "**5)\tImplement a basic multiple perceptron for classification on XOR & XNOR truth table**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tUqaDMtFNwe",
        "outputId": "bc0d9a52-7d18-47bb-b4cc-64896b38b198"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# XOR truth table\n",
        "X_xor = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "y_xor = [0, 1, 1, 0]\n",
        "\n",
        "# XNOR truth table\n",
        "X_xnor = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "y_xnor = [1, 0, 0, 1]\n",
        "\n",
        "# Create and train MLP classifier for XOR\n",
        "mlp_xor = MLPClassifier(hidden_layer_sizes=(4, 2), activation='relu', max_iter=2000)\n",
        "mlp_xor.fit(X_xor, y_xor)\n",
        "\n",
        "# Create and train MLP classifier for XNOR\n",
        "mlp_xnor = MLPClassifier(hidden_layer_sizes=(4, 2), activation='relu', max_iter=2000)\n",
        "mlp_xnor.fit(X_xnor, y_xnor)\n",
        "\n",
        "# Predictions\n",
        "print(\"Predictions for XOR:\")\n",
        "print(\"0 XOR 0:\", mlp_xor.predict([[0, 0]]))\n",
        "print(\"0 XOR 1:\", mlp_xor.predict([[0, 1]]))\n",
        "print(\"1 XOR 0:\", mlp_xor.predict([[1, 0]]))\n",
        "print(\"1 XOR 1:\", mlp_xor.predict([[1, 1]]))\n",
        "\n",
        "print(\"\\nPredictions for XNOR:\")\n",
        "print(\"0 XNOR 0:\", mlp_xnor.predict([[0, 0]]))\n",
        "print(\"0 XNOR 1:\", mlp_xnor.predict([[0, 1]]))\n",
        "print(\"1 XNOR 0:\", mlp_xnor.predict([[1, 0]]))\n",
        "print(\"1 XNOR 1:\", mlp_xnor.predict([[1, 1]]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsUOUegIFaEp"
      },
      "source": [
        "\n",
        "**6)Implement a basic multiple perceptron for classification on AND & NAND truth table**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YJ3XfEGFcws",
        "outputId": "e0fb8323-3e4a-4510-e83e-644a4d58156c"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# AND truth table\n",
        "X_and = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "y_and = [0, 0, 0, 1]\n",
        "\n",
        "# NAND truth table\n",
        "X_nand = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "y_nand = [1, 1, 1, 0]\n",
        "\n",
        "# Create and train MLP classifier for AND\n",
        "mlp_and = MLPClassifier(hidden_layer_sizes=(2,), activation='logistic', max_iter=50000)\n",
        "mlp_and.fit(X_and, y_and)\n",
        "\n",
        "# Create and train MLP classifier for NAND\n",
        "mlp_nand = MLPClassifier(hidden_layer_sizes=(2,), activation='logistic', max_iter=50000)\n",
        "mlp_nand.fit(X_nand, y_nand)\n",
        "\n",
        "# Predictions\n",
        "print(\"Predictions for AND:\")\n",
        "print(\"0 AND 0:\", mlp_and.predict([[0, 0]]))\n",
        "print(\"0 AND 1:\", mlp_and.predict([[0, 1]]))\n",
        "print(\"1 AND 0:\", mlp_and.predict([[1, 0]]))\n",
        "print(\"1 AND 1:\", mlp_and.predict([[1, 1]]))\n",
        "\n",
        "print(\"\\nPredictions for NAND:\")\n",
        "print(\"0 NAND 0:\", mlp_nand.predict([[0, 0]]))\n",
        "print(\"0 NAND 1:\", mlp_nand.predict([[0, 1]]))\n",
        "print(\"1 NAND 0:\", mlp_nand.predict([[1, 0]]))\n",
        "print(\"1 NAND 1:\", mlp_nand.predict([[1, 1]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ4622sFFvhW"
      },
      "source": [
        "**7)\tBuild a Basic neural network from scratch using a high level library like tensorflow or Pytorch for prediction. Use appropriate dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DTNvCr_FjPu",
        "outputId": "ee52922f-8eb5-443c-c6a0-add770f09e53"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load your own dataset\n",
        "# Replace 'path_to_your_dataset.csv' with the actual path to your dataset file\n",
        "data = pd.read_csv('./src/BostonHousing.csv')\n",
        "\n",
        "# Specify the target column name\n",
        "target_column_name = 'medv'\n",
        "\n",
        "# Split the dataset into features and target variable\n",
        "X = data.drop(columns=[target_column_name])\n",
        "y = data[target_column_name]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build the neural network model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=16, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, mae = model.evaluate(X_test_scaled, y_test)\n",
        "print(f\"Test MAE: {mae}\")\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_test_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udEjCYzmHTkI"
      },
      "source": [
        "**9)\tOptimize hyperparameters for a neural network model. Implement a hyperparameter optimization strategy and compare the performance with different hyperparameter configurations for classification (using keras tuner)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdFfayNhIqhh",
        "outputId": "a087ec0d-b1f1-446f-db9d-7da154feb4fe"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from kerastuner.tuners import RandomSearch\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model-building function\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Input(shape=(4,)))  # Input layer\n",
        "\n",
        "    # Tune the number of units in the first Dense layer\n",
        "    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "    model.add(layers.Dense(units=hp_units, activation='relu'))\n",
        "\n",
        "    # Tune the number of Dense layers\n",
        "    hp_layers = hp.Int('num_layers', min_value=1, max_value=3, step=1)\n",
        "    for _ in range(hp_layers):\n",
        "        model.add(layers.Dense(units=hp_units, activation='relu'))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define the tuner\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=5,\n",
        "    executions_per_trial=3,\n",
        "    directory='tuner_results',\n",
        "    project_name='iris_classification'\n",
        ")\n",
        "\n",
        "# Search for the best hyperparameter configuration\n",
        "tuner.search(X_train, y_train,\n",
        "             epochs=10,  # Number of epochs to train each model\n",
        "             validation_data=(X_test, y_test))\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Build the best model with the optimal hyperparameters\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Train the best model\n",
        "best_model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvYXT1WFHgMA"
      },
      "source": [
        "**10)\tOptimize hyperparameters for a neural network model. Implement a hyperparameter optimization strategy and compare the performance with different hyperparameter configurations for regression (using keras tuner)\n",
        "Boston Dataset:- https://github.com/selva86/datasets/blob/master/BostonHousing.csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loqlgT6ZIsSv",
        "outputId": "28021157-9cf3-47aa-e52f-81c8ff586cb3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from kerastuner.tuners import RandomSearch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = pd.read_csv('./src/BostonHousing.csv')\n",
        "\n",
        "X = data.drop(columns=['medv'])\n",
        "y = data['medv']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Input(shape=(X_train_scaled.shape[1],)))\n",
        "\n",
        "    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "    model.add(layers.Dense(units=hp_units, activation='relu'))\n",
        "\n",
        "    hp_layers = hp.Int('num_layers', min_value=1, max_value=3, step=1)\n",
        "    for _ in range(hp_layers):\n",
        "        model.add(layers.Dense(units=hp_units, activation='relu'))\n",
        "\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
        "                  loss='mse',\n",
        "                  metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_mae',\n",
        "    max_trials=5,\n",
        "    executions_per_trial=3,\n",
        "    directory='my_dir',\n",
        "    project_name='boston_housing_regression'\n",
        ")\n",
        "\n",
        "tuner.search(X_train_scaled, y_train,\n",
        "             epochs=10,\n",
        "             validation_data=(X_test_scaled, y_test))\n",
        "\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "best_model.fit(X_train_scaled, y_train, epochs=10, validation_data=(X_test_scaled, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy4FRTSIHpX3"
      },
      "source": [
        "**11)\tOptimize hyperparameters for a neural network model. Implement a hyperparameter optimization strategy and compare the performance with different hyperparameter configurations for classification (without keras tuner)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train_nn(hidden_layer_sizes, alpha, max_iter):\n",
        "    model = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, alpha=alpha, max_iter=max_iter, random_state=42)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return accuracy\n",
        "\n",
        "hidden_layer_sizes_list = [(10,), (20,), (30,), (40,)]\n",
        "alpha_list = [0.0001, 0.001, 0.01]\n",
        "max_iter_list = [100, 200, 300]\n",
        "\n",
        "best_accuracy = 0\n",
        "best_params = None\n",
        "\n",
        "for hidden_layer_sizes in hidden_layer_sizes_list:\n",
        "    for alpha in alpha_list:\n",
        "        for max_iter in max_iter_list:\n",
        "            accuracy = train_nn(hidden_layer_sizes, alpha, max_iter)\n",
        "            print(f\"Hidden Layer Sizes: {hidden_layer_sizes}, Alpha: {alpha}, Max Iter: {max_iter}, Accuracy: {accuracy}\")\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_params = (hidden_layer_sizes, alpha, max_iter)\n",
        "\n",
        "print(f\"\\nBest Accuracy: {best_accuracy*100:.2f}%\")\n",
        "print(f\"Best Hyperparameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1bIPNJCHrfW"
      },
      "source": [
        "**12)\tOptimize hyperparameters for a neural network model. Implement a hyperparameter optimization strategy and compare the performance with different hyperparameter configurations for regression (without keras tuner)\n",
        "Boston Dataset:- https://github.com/selva86/datasets/blob/master/BostonHousing.csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMnToBmeItIL",
        "outputId": "8a654197-4175-47fd-cdef-1a4ae47e9a8e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Load the Boston dataset from your machine\n",
        "# Replace 'path_to_your_dataset.csv' with the actual path to your dataset file\n",
        "data = pd.read_csv('/content/BostonHousing.csv')\n",
        "\n",
        "# Split the dataset into features and target variable\n",
        "X = data.drop(columns=['medv'])\n",
        "y = data['medv']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the neural network model\n",
        "def create_model(hidden_units=64, activation='relu', optimizer='adam'):\n",
        "    model = Sequential([\n",
        "        Dense(hidden_units, activation=activation, input_shape=(X_train_scaled.shape[1],)),\n",
        "        Dense(hidden_units, activation=activation),\n",
        "        Dense(1)  # Output layer for regression\n",
        "    ])\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Define hyperparameters to search\n",
        "param_grid = {\n",
        "    'hidden_units': [32, 64, 128],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'optimizer': ['adam', 'sgd']\n",
        "}\n",
        "\n",
        "# Create the model\n",
        "model = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=create_model, epochs=10, batch_size=16, verbose=0)\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
        "grid_result = grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_result.best_params_\n",
        "\n",
        "# Create the best model with the optimal hyperparameters\n",
        "best_model = create_model(hidden_units=best_params['hidden_units'],\n",
        "                          activation=best_params['activation'],\n",
        "                          optimizer=best_params['optimizer'])\n",
        "\n",
        "# Train the best model\n",
        "best_model.fit(X_train_scaled, y_train, epochs=10, batch_size=16, verbose=0, validation_data=(X_test_scaled, y_test))\n",
        "\n",
        "# Evaluate the best model\n",
        "test_loss = best_model.evaluate(X_test_scaled, y_test)\n",
        "\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "print(f\"Test Loss: {test_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVOrRbrpHufP"
      },
      "source": [
        "**13)\tCompute the Hessian matrix for a given scalar-valued function of multiple variables.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvczj-gpItwe",
        "outputId": "ab6604b8-d77a-4062-99ec-b1336a38d0d2"
      },
      "outputs": [],
      "source": [
        "import sympy as sp\n",
        "\n",
        "x, y = sp.symbols('x y')\n",
        "\n",
        "f = x**2 + 2*x*y + y**2\n",
        "\n",
        "f_x = sp.diff(f, x)\n",
        "f_y = sp.diff(f, y)\n",
        "\n",
        "f_xx = sp.diff(f_x, x)\n",
        "f_xy = sp.diff(f_x, y)\n",
        "f_yx = sp.diff(f_y, x)\n",
        "f_yy = sp.diff(f_y, y)\n",
        "\n",
        "hessian_matrix = sp.Matrix([\n",
        "    [f_xx, f_xy],\n",
        "    [f_yx, f_yy]\n",
        "])\n",
        "\n",
        "print(\"Hessian Matrix:\")\n",
        "print(hessian_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtuV_RzeH1eF"
      },
      "source": [
        "**14)\tImplement Bayesian methods for classification using Gaussian**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VnemIh8WIuw7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9216</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,179,776</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9216\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m1,179,776\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,199,882</span> (4.58 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,199,882\u001b[0m (4.58 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,199,882</span> (4.58 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,199,882\u001b[0m (4.58 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 13ms/step - accuracy: 0.7750 - loss: 0.6331\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - accuracy: 0.8826 - loss: 0.3270\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 13ms/step - accuracy: 0.9009 - loss: 0.2701\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - accuracy: 0.9097 - loss: 0.2440\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - accuracy: 0.9197 - loss: 0.2192\n",
            "(10000, 28, 28, 1)\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9172 - loss: 0.2434\n",
            "Test accuracy: 91.62%\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
        "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)\n",
        "\n",
        "def preprocess_images(imgs):\n",
        "    sample_img = imgs if len(imgs.shape) == 2 else imgs[0]\n",
        "    assert sample_img.shape in [(28, 28, 1), (28, 28)], sample_img.shape\n",
        "    return imgs / 255.0\n",
        "\n",
        "train_images = preprocess_images(train_images)\n",
        "test_images = preprocess_images(test_images)\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_images, train_labels, epochs=5)\n",
        "\n",
        "print(test_images.shape)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "\n",
        "print(f'Test accuracy: {test_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZNb5MI7H4Zf"
      },
      "source": [
        "**15)\tImplement Bayesian methods for classification using Naïve Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Lwe0pdW8Ivel"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 96.67%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
        "\n",
        "clf = GaussianNB()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzRmC2WUIOBD"
      },
      "source": [
        "**16)\tImplement Principal component analysis (02 components) for dimensionality reduction of data points**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0o37FlPIv27"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "target_names = iris.target_names\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "n_components = 2\n",
        "pca = PCA(n_components=n_components)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "plt.figure()\n",
        "colors = ['navy', 'turquoise', 'darkorange']\n",
        "lw = 2\n",
        "\n",
        "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
        "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], color=color, alpha=.8, lw=lw,\n",
        "                label=target_name)\n",
        "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
        "plt.title('PCA of IRIS dataset')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86ljnEJxIVwq"
      },
      "source": [
        "**17)\tImplement Principal component analysis (04 components) for dimensionality reduction of data points**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZUcWBYPIwVA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "target_names = cancer.target_names\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "n_components = 4 \n",
        "pca = PCA(n_components=n_components)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "for color, i, target_name in zip(['navy', 'turquoise'], [0, 1], target_names):\n",
        "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], color=color, alpha=.8, lw=2,\n",
        "                label=target_name)\n",
        "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
        "plt.title('PCA of Breast Cancer Dataset')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em304A9rIadw"
      },
      "source": [
        "**18)\tImplement Hidden Markov Model for sequence prediction and evaluate model performance(Stock Market)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "IondcGBAIxQT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%%**********************]  1 of 1 completed"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Score (Likelihood): 2021.8568281442156\n",
            "Test Score (Likelihood): 102.3171745577145\n"
          ]
        }
      ],
      "source": [
        "import yfinance as yf\n",
        "import numpy as np\n",
        "from hmmlearn import hmm\n",
        "\n",
        "# Fetch stock data using yfinance\n",
        "ticker = \"AAPL\"  # Apple Inc.\n",
        "data = yf.download(ticker, start=\"2020-01-01\", end=\"2023-04-30\")\n",
        "\n",
        "# Preprocess the data\n",
        "close_prices = data[\"Adj Close\"].values\n",
        "close_prices = close_prices.reshape(-1, 1)\n",
        "\n",
        "# Discretize the data\n",
        "num_bins = 5  # Number of discrete states\n",
        "states = np.linspace(close_prices.min(), close_prices.max(), num_bins)\n",
        "close_prices_discretized = np.digitize(close_prices, states)\n",
        "\n",
        "# Create an HMM model\n",
        "num_components = 5  # Number of hidden states\n",
        "model = hmm.GaussianHMM(n_components=num_components, covariance_type=\"diag\")\n",
        "\n",
        "# Train the HMM model\n",
        "model.fit(close_prices_discretized)\n",
        "\n",
        "# Generate predictions\n",
        "num_predictions = 30  # Number of future days to predict\n",
        "predictions = model.sample(num_predictions)[0]\n",
        "\n",
        "# Evaluate model performance\n",
        "last_train_day = len(close_prices_discretized) - num_predictions\n",
        "train_scores = model.score(close_prices_discretized[:last_train_day])\n",
        "test_scores = model.score(close_prices_discretized[last_train_day:])\n",
        "\n",
        "print(f\"Train Score (Likelihood): {train_scores}\")\n",
        "print(f\"Test Score (Likelihood): {test_scores}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65X71J1SIdqB"
      },
      "source": [
        "**19)\tImplement Hidden Markov Model for sequence prediction and evaluate model performance(Weather prediction)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlttHHtEIx0E"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from hmmlearn import hmm\n",
        "\n",
        "# Load weather data\n",
        "weather_data = pd.read_csv(\"seattle-weather.csv\")\n",
        "\n",
        "# Preprocess the data\n",
        "observations = weather_data[[\"temp_max\", \"temp_min\", \"wind\", \"precipitation\"]].values\n",
        "\n",
        "# Discretize the data\n",
        "num_bins = 5  # Number of discrete states\n",
        "states = np.linspace(observations.min(), observations.max(), num_bins)\n",
        "observations_discretized = np.digitize(observations, states)\n",
        "\n",
        "# Create an HMM model\n",
        "num_components = 3  # Number of hidden states\n",
        "model = hmm.MultinomialHMM(n_components=num_components)\n",
        "\n",
        "# Train the HMM model\n",
        "model.fit(observations_discretized)\n",
        "\n",
        "# Generate predictions\n",
        "num_predictions = 30  # Number of future days to predict\n",
        "predicted_states = model.predict(observations_discretized[-1].reshape(1, -1))\n",
        "\n",
        "# Define a mapping of numerical states to weather conditions\n",
        "weather_conditions = {\n",
        "    0: \"Sunny\",\n",
        "    1: \"Cloudy\",\n",
        "    2: \"Rainy\",\n",
        "    # Add more mappings if needed\n",
        "}\n",
        "\n",
        "# Map numerical states to weather conditions\n",
        "predicted_weather = [weather_conditions[state] for state in predicted_states]\n",
        "\n",
        "# Print predictions\n",
        "print(\"Predicted Weather:\")\n",
        "for weather in predicted_weather:\n",
        "    print(f\"Predicted weather condition: {weather}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx2lLTLHIg-o"
      },
      "source": [
        "**20)\tImplement Hidden Markov Model for sequence prediction and evaluate model performance(Supply chain )**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zShYRaeFzv3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from hmmlearn import hmm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load supply chain data\n",
        "supply_chain_data = pd.read_csv(\"supply_chain_data.csv\")\n",
        "\n",
        "# Preprocess the data\n",
        "# For this example, let's focus on predicting future sales volume based on past sales\n",
        "sales_data = supply_chain_data[[\"Number of products sold\"]].values\n",
        "\n",
        "# Discretize the data\n",
        "num_bins = 5  # Number of discrete states\n",
        "states = np.linspace(sales_data.min(), sales_data.max(), num_bins)\n",
        "sales_data_discretized = np.digitize(sales_data, states)\n",
        "\n",
        "# Create an HMM model\n",
        "num_components = 3  # Number of hidden states\n",
        "model = hmm.MultinomialHMM(n_components=num_components)\n",
        "\n",
        "# Train the HMM model\n",
        "model.fit(sales_data_discretized)\n",
        "\n",
        "# Generate predictions\n",
        "num_predictions = 30  # Number of future time steps to predict\n",
        "predicted_states = model.predict(sales_data_discretized[-1].reshape(1, -1))\n",
        "predicted_sales = states[predicted_states]\n",
        "\n",
        "# Print predictions\n",
        "print(\"Predicted Sales Volume:\")\n",
        "print(predicted_sales)\n",
        "\n",
        "# Note: Depending on the dataset and its features, you may need to adjust the preprocessing and modeling steps accordingly."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
